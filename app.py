import streamlit as st
import graphviz
import os
import google.generativeai as genai

from data import TOPOLOGY
from logic import CausalInferenceEngine, Alarm
from network_ops import run_diagnostic_commands 

st.set_page_config(page_title="Antigravity Live", page_icon="âš¡", layout="wide")

# --- ãƒˆãƒãƒ­ã‚¸ãƒ¼æç”» ---
def render_topology(alarms, root_cause_node):
    graph = graphviz.Digraph()
    graph.attr(rankdir='TB')
    graph.attr('node', shape='box', style='rounded,filled', fontname='Helvetica')
    alarmed_ids = {a.device_id for a in alarms}
    for node_id, node in TOPOLOGY.items():
        color = "#e8f5e9"
        penwidth = "1"
        if root_cause_node and node_id == root_cause_node.id:
            color = "#ffcdd2"
            penwidth = "3"
        elif node_id in alarmed_ids:
            color = "#fff9c4"
        graph.node(node_id, label=f"{node_id}\n({node.type})", fillcolor=color, color='black', penwidth=penwidth)
    for node_id, node in TOPOLOGY.items():
        if node.parent_id:
            graph.edge(node.parent_id, node_id)
            parent = TOPOLOGY.get(node.parent_id)
            if parent and parent.redundancy_group:
                partners = [n.id for n in TOPOLOGY.values() if n.redundancy_group == parent.redundancy_group and n.id != parent.id]
                for p in partners: graph.edge(p, node_id)
    return graph

# --- Configèª­ã¿è¾¼ã¿ ---
def load_config_by_id(device_id):
    path = f"configs/{device_id}.txt"
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f: return f.read()
    return None

# --- UIæ§‹ç¯‰ ---
st.title("âš¡ Antigravity AI Agent (Live Demo)")

api_key = None
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = os.environ.get("GOOGLE_API_KEY")

with st.sidebar:
    st.header("âš¡ é‹ç”¨ãƒ¢ãƒ¼ãƒ‰é¸æŠ")
    selected_scenario = st.radio(
        "ã‚·ãƒŠãƒªã‚ª:", 
        ("æ­£å¸¸ç¨¼åƒ", "1. WANå…¨å›ç·šæ–­", "2. FWç‰‡ç³»éšœå®³", "3. L2SWã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³", "4. [Live] Ciscoå®Ÿæ©Ÿè¨ºæ–­")
    )
    if not api_key:
        st.warning("API Key Missing")
        user_key = st.text_input("Google API Key", type="password")
        if user_key: api_key = user_key

if "current_scenario" not in st.session_state:
    st.session_state.current_scenario = "æ­£å¸¸ç¨¼åƒ"
    st.session_state.messages = []
    st.session_state.chat_session = None 
    st.session_state.live_result = None

if st.session_state.current_scenario != selected_scenario:
    st.session_state.current_scenario = selected_scenario
    st.session_state.messages = []
    st.session_state.chat_session = None
    st.session_state.live_result = None
    st.rerun()

# --- ãƒ¢ãƒ¼ãƒ‰åˆ†å² ---
if selected_scenario != "4. [Live] Ciscoå®Ÿæ©Ÿè¨ºæ–­":
    # === ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ ===
    alarms = []
    if selected_scenario == "1. WANå…¨å›ç·šæ–­":
        alarms = [Alarm("WAN_ROUTER_01", "Down", "CRITICAL"), Alarm("FW_01_PRIMARY", "Unreach", "WARNING"), Alarm("AP_01", "Unreach", "CRITICAL")]
    elif selected_scenario == "2. FWç‰‡ç³»éšœå®³":
        alarms = [Alarm("FW_01_PRIMARY", "HB Loss", "WARNING")]
    elif selected_scenario == "3. L2SWã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³":
        alarms = [Alarm("AP_01", "Lost", "CRITICAL"), Alarm("AP_02", "Lost", "CRITICAL")]

    root_cause = None
    reason = ""
    if alarms:
        engine = CausalInferenceEngine(TOPOLOGY)
        res = engine.analyze_alarms(alarms)
        root_cause = res.root_cause_node
        reason = res.root_cause_reason

    col1, col2 = st.columns([1, 1])
    with col1:
        st.subheader("Topology")
        st.graphviz_chart(render_topology(alarms, root_cause), use_container_width=True)
        if root_cause:
            st.markdown(f'<div style="color:#d32f2f;background:#fdecea;padding:10px;border-radius:5px;">ğŸš¨ ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆï¼š{root_cause.id} ãƒ€ã‚¦ãƒ³</div>', unsafe_allow_html=True)
            st.caption(f"ç†ç”±: {reason}")

    with col2:
        st.subheader("AI Chat")
        if not api_key: st.stop()
        
        # ã‚³ãƒ³ãƒ†ãƒŠã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›ºå®š
        chat_container = st.container(height=500)
        
        if st.session_state.chat_session is None and selected_scenario != "æ­£å¸¸ç¨¼åƒ":
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel("gemini-2.0-flash", generation_config={"temperature": 0.0})
            
            conf = load_config_by_id(root_cause.id)
            sys_prompt = f"éšœå®³: {root_cause.id}ã€‚ç†ç”±: {reason}ã€‚"
            if conf: sys_prompt += f"\nConfig:\n{conf}"
            else: sys_prompt += "\nConfigãªã—ã€‚"
            
            history = [{"role": "user", "parts": [sys_prompt + "\nçŠ¶æ³å ±å‘Šã¨ãƒã‚¯ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æç¤ºã›ã‚ˆã€‚"]}]
            chat = model.start_chat(history=history)
            try:
                response = chat.send_message("çŠ¶æ³å ±å‘Š")
                st.session_state.chat_session = chat
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e: st.error(str(e))

        with chat_container:
            for msg in st.session_state.messages:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if prompt := st.chat_input("æŒ‡ç¤ºã‚’å…¥åŠ›..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with chat_container:
                with st.chat_message("user"): st.markdown(prompt)
            if st.session_state.chat_session:
                with chat_container:
                    with st.chat_message("assistant"):
                        with st.spinner("Thinking..."):
                            res = st.session_state.chat_session.send_message(prompt)
                            st.markdown(res.text)
                            st.session_state.messages.append({"role": "assistant", "content": res.text})

else:
    # === Liveå®Ÿæ©Ÿãƒ¢ãƒ¼ãƒ‰ ===
    st.subheader("ğŸŒ Real-world Network Diagnostic")
    c1, c2 = st.columns([1, 1])
    
    with c1:
        st.info("Target: sandbox-iosxe-latest-1.cisco.com")
        if st.button("ğŸš€ è‡ªå¾‹èª¿æŸ»å®Ÿè¡Œ (SSH)", type="primary"):
            if not api_key: st.error("API Key Required")
            else:
                with st.status("Autopilot Running...") as status:
                    st.write("Connecting to Cisco Sandbox...")
                    res = run_diagnostic_commands()
                    st.session_state.live_result = res
                    
                    if res["status"] == "SUCCESS":
                        status.update(label="Complete!", state="complete")
                    else:
                        # ã‚¨ãƒ©ãƒ¼æ™‚
                        status.update(label="Failed", state="error")
                        # â˜…ã“ã“ã‚’è¿½åŠ ï¼šè©³ç´°ãªã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’èµ¤å­—ã§è¡¨ç¤º
                        st.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {res['error']}")        
        if st.session_state.live_result and st.session_state.live_result["status"] == "SUCCESS":
            with st.expander("å–å¾—ãƒ­ã‚° (Sanitized)", expanded=True):
                st.code(st.session_state.live_result["sanitized_log"])

    with c2:
        st.subheader("AI Analysis")
        if st.session_state.live_result and st.session_state.live_result["status"] == "SUCCESS":
            chat_container = st.container(height=500)
            
            if st.session_state.chat_session is None:
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel("gemini-2.0-flash", generation_config={"temperature": 0.0})
                log = st.session_state.live_result["sanitized_log"]
                sys_prompt = f"ä»¥ä¸‹ã¯Ciscoå®Ÿæ©Ÿãƒ­ã‚°ã§ã™ã€‚åˆ†æã—ã¦ãã ã•ã„ã€‚\n{log}"
                history = [{"role": "user", "parts": [sys_prompt]}]
                chat = model.start_chat(history=history)
                with st.spinner("Analyzing..."):
                    res = chat.send_message("ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ")
                    st.session_state.chat_session = chat
                    st.session_state.messages.append({"role": "assistant", "content": res.text})

            with chat_container:
                for msg in st.session_state.messages:
                    with st.chat_message(msg["role"]): st.markdown(msg["content"])
            
            if prompt := st.chat_input("è³ªå•..."):
                st.session_state.messages.append({"role": "user", "content": prompt})
                with chat_container:
                    with st.chat_message("user"): st.markdown(prompt)
                with chat_container:
                    with st.chat_message("assistant"):
                        with st.spinner("Thinking..."):
                            res = st.session_state.chat_session.send_message(prompt)
                            st.markdown(res.text)
                            st.session_state.messages.append({"role": "assistant", "content": res.text})
